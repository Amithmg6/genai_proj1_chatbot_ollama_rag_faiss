{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fc39ef1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6c9266eb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16acb4cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e5ed697",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "import glob # To list PDF files\n",
    "import PyPDF2 # Required for viewing PDF content\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9ef9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Welcome to Chat with Machine Learning Encyclopedia.\\n\")\n",
    "print(\"Generative AI project with RAG and LLM.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7d1962",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Configuration Section ---\n",
    "print(\"\\n# --- Configuring the LLM and Embeddings ---#\")\n",
    "\n",
    "# Function to let user choose an embedding model\n",
    "def choose_embedding_model():\n",
    "    \"\"\"Prompts the user to select an embedding model.\"\"\"\n",
    "    embedding_models = {\n",
    "        \"1\": \"all-MiniLM-L6-v2\",\n",
    "        \"2\": \"all-mpnet-base-v2\",\n",
    "        \"3\": \"nomic-embed-text\" # Popular choice for Ollama embeddings\n",
    "    }\n",
    "    print(\"\\nChoose an Embedding Model:\")\n",
    "    for key, value in embedding_models.items():\n",
    "        print(f\"{key}. {value}\")\n",
    "\n",
    "    while True:\n",
    "        choice = input(\"Enter the number of your desired embedding model: \")\n",
    "        if choice in embedding_models:\n",
    "            return embedding_models[choice]\n",
    "        else:\n",
    "            print(\"Invalid choice. Please select a valid number.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8890f121",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set global configuration based on user input for main execution\n",
    "# This will now be a list of paths\n",
    "BOOK_PATHS = choose_pdf_file() # Now it returns a list of selected PDF paths\n",
    "if not BOOK_PATHS:\n",
    "    exit(\"No PDFs selected. Exiting.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d732aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "OLLAMA_MODEL_NAME = \"gemma:2b\" # Still hardcoded to gemma:2b, but can be made user-selectable\n",
    "EMBEDDING_MODEL_NAME = choose_embedding_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc049b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# For FAISS index path, create a generic one since multiple PDFs are used\n",
    "FAISS_INDEX_PATH = \"faiss_index_multi_pdf\"\n",
    "\n",
    "print(f\"\\nSelected PDF(s): {', '.join(BOOK_PATHS)}\")\n",
    "print(f\"Selected Embedding Model: {EMBEDDING_MODEL_NAME}\")\n",
    "print(f\"Ollama LLM Model: {OLLAMA_MODEL_NAME}\")\n",
    "print(f\"FAISS Index Path: {FAISS_INDEX_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c439a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- 1. Data Preparation ---\n",
    "print(\"\\n--- 1. Data Preparation ---\")\n",
    "\n",
    "def load_and_chunk_documents(book_path: str):\n",
    "    \"\"\"\n",
    "    Loads a PDF document and splits it into manageable chunks.\n",
    "    These chunks are used for creating embeddings and retrieving context.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        loader = PyPDFLoader(book_path)\n",
    "        documents = loader.load()\n",
    "        print(f\"Loaded {len(documents)} pages from '{os.path.basename(book_path)}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading PDF: {e}\")\n",
    "        print(\"Please ensure the book path is correct and the PDF is not corrupted.\")\n",
    "        return []\n",
    "\n",
    "    # Defines how to split the text: chunk_size is max characters per chunk,\n",
    "    # chunk_overlap ensures continuity between chunks.\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=2000,\n",
    "        chunk_overlap=300,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False,\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    print(f\"Split book into {len(chunks)} chunks.\")\n",
    "    return chunks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae1bb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 2. Create Embeddings and Vector Store ---\n",
    "print(\"\\n--- 2. Create Embeddings and Vector Store ---\")\n",
    "\n",
    "def create_vector_store(chunks, embedding_model_name: str, faiss_path: str):\n",
    "    \"\"\"\n",
    "    Creates or loads a FAISS vector store.\n",
    "    FAISS stores vector embeddings of text chunks for efficient similarity search.\n",
    "    If an index already exists, it loads it; otherwise, it creates and saves a new one.\n",
    "    \"\"\"\n",
    "    embeddings = SentenceTransformerEmbeddings(model_name=embedding_model_name)\n",
    "\n",
    "    if os.path.exists(faiss_path):\n",
    "        print(f\"Loading existing vector store from '{faiss_path}'...\")\n",
    "        # allow_dangerous_deserialization=True is needed for loading local FAISS indexes\n",
    "        vectorstore = FAISS.load_local(faiss_path, embeddings, allow_dangerous_deserialization=True)\n",
    "        print(\"Vector store loaded.\")\n",
    "    else:\n",
    "        print(\"Creating new vector store (this might take a moment)...\")\n",
    "        vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "        vectorstore.save_local(faiss_path)\n",
    "        print(\"Vector store created and saved.\")\n",
    "    return vectorstore\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79af79d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 3. Implement RAG Chain ---\n",
    "print(\"\\n--- 3. Implement RAG Chain ---\")\n",
    "\n",
    "def setup_rag_chain(llm_model_name: str, retriever):\n",
    "    \"\"\"\n",
    "    Sets up the Retrieval Augmented Generation (RAG) chain.\n",
    "    This chain connects the LLM with the vector store (retriever)\n",
    "    to answer questions using context from the documents.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize the Ollama LLM (ensure Ollama server is running)\n",
    "        llm = Ollama(model=llm_model_name)\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing Ollama with model '{llm_model_name}': {e}\")\n",
    "        print(\"Please ensure the Ollama server is running and the model is pulled (e.g., 'ollama pull gemma:2b').\")\n",
    "        return None\n",
    "\n",
    "    # Define the prompt template for the RAG chain\n",
    "    # {context} will be filled by retrieved document chunks\n",
    "    # {question} will be the user's query\n",
    "    rag_prompt_template = \"\"\"Use the following context to answer the question at the end.\n",
    "If you don't know the answer, state that you don't know, and do not make up an answer.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    RAG_PROMPT = PromptTemplate(\n",
    "        template=rag_prompt_template,\n",
    "        input_variables=[\"context\", \"question\"]\n",
    "    )\n",
    "\n",
    "    # Create the RetrievalQA chain\n",
    "    # chain_type=\"stuff\" means all retrieved documents are \"stuffed\" into the prompt.\n",
    "    # return_source_documents=True allows seeing which parts of the PDF were used.\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=retriever,\n",
    "        return_source_documents=True,\n",
    "        chain_type_kwargs={\"prompt\": RAG_PROMPT}\n",
    "    )\n",
    "    return qa_chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9eaa48",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Main Chatbot Logic ---\n",
    "print(\"\\n--- Chatbot Loading ---\\n\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the chatbot.\"\"\"\n",
    "    print(\"Initializing Chatbot...\")\n",
    "\n",
    "    # Step 1: Load and chunk documents from potentially multiple PDFs\n",
    "    all_chunks = []\n",
    "    for book_path in BOOK_PATHS:\n",
    "        print(f\"\\nProcessing: {os.path.basename(book_path)}\")\n",
    "        chunks_from_pdf = load_and_chunk_documents(book_path)\n",
    "        if chunks_from_pdf: # Only extend if chunks were successfully loaded\n",
    "            all_chunks.extend(chunks_from_pdf)\n",
    "    \n",
    "    if not all_chunks:\n",
    "        print(\"Exiting due to no content loaded from selected PDFs.\")\n",
    "        return\n",
    "\n",
    "    # Step 2: Create/Load vector store with chosen embedding model using all chunks\n",
    "    vectorstore = create_vector_store(all_chunks, EMBEDDING_MODEL_NAME, FAISS_INDEX_PATH)\n",
    "    # Configure the retriever to fetch the top 4 most relevant chunks\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "\n",
    "    # Step 3: Set up the RAG chain with the Ollama LLM\n",
    "    qa_chain = setup_rag_chain(OLLAMA_MODEL_NAME, retriever)\n",
    "    if not qa_chain:\n",
    "        print(\"Exiting due to RAG chain setup error.\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nChatbot initialized with '{OLLAMA_MODEL_NAME}' and '{EMBEDDING_MODEL_NAME}' embeddings, using {len(BOOK_PATHS)} PDF(s)!\")\n",
    "    print(\"Type 'exit' to quit.\")\n",
    "\n",
    "    # --- Chat Loop ---\n",
    "    while True:\n",
    "        user_query = input(\"\\nYour Question: \")\n",
    "        if user_query.lower() == 'exit':\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "\n",
    "        print(\"Chatbot thinking...\")\n",
    "        try:\n",
    "            # Invoke the RAG chain with the user's query\n",
    "            result = qa_chain.invoke({\"query\": user_query})\n",
    "            chatbot_response = result[\"result\"]\n",
    "            source_documents = result.get(\"source_documents\", [])\n",
    "\n",
    "            print(\"\\n--- Chatbot Response ---\")\n",
    "            print(chatbot_response)\n",
    "\n",
    "            if source_documents:\n",
    "                print(\"\\n--- Sources Used (Top Retrieved Chunks) ---\")\n",
    "                for i, doc in enumerate(source_documents):\n",
    "                    # Display page number if available in document metadata\n",
    "                    page_info = f\"Page: {doc.metadata.get('page') + 1}, \" if 'page' in doc.metadata else \"\"\n",
    "                    source_filename = os.path.basename(doc.metadata.get('source', 'Unknown'))\n",
    "                    print(f\"Chunk {i+1} ({page_info}Source: {source_filename}):\")\n",
    "                    # Display first 200 characters of the content for brevity\n",
    "                    print(f\"  {doc.page_content[:200]}...\")\n",
    "            print(\"------------------------\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred during response generation: {e}\")\n",
    "            print(\"Please check your Ollama server and model, and ensure your prompt is well-formed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27318081",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1fc577",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8189140b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0505e94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
