{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f05b9b99",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "70d820f3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e6936a8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cfc6eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33132a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Configuration (Adjust these paths and model names) ---\n",
    "# Replace with the actual path to your technical book (e.g., \"C:/Users/YourUser/Documents/my_tech_book.pdf\")\n",
    "BOOK_PATH = \"Encyclopedia of Machine Learning.pdf\" \n",
    "# Choose one of the small LLMs you pulled with Ollama (e.g., \"gemma:2b\", \"phi3\", \"qwen:3b\", \"mistral\")\n",
    "OLLAMA_MODEL_NAME = \"gemma:2b\" \n",
    "# Embedding model (all-MiniLM-L6-v2 is a good default, or \"nomic-embed-text\" if using Ollama for embeddings)\n",
    "EMBEDDING_MODEL_NAME = \"all-mpnet-base-v2\" # \"all-MiniLM-L6-v2\"\n",
    "# Path to save/load your FAISS vector store to avoid re-embedding\n",
    "FAISS_INDEX_PATH = \"faiss_index_tech_book\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "208ed64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# model = SentenceTransformer(\"intfloat/e5-mistral-7b-instruct\")\n",
    "# # In case you want to reduce the maximum sequence length:\n",
    "# model.max_seq_length = 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abf913b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 1. Data Preparation ---\n",
    "def load_and_chunk_documents(book_path: str):\n",
    "    \"\"\"Loads a PDF document and splits it into chunks.\"\"\"\n",
    "    try:\n",
    "        loader = PyPDFLoader(book_path)\n",
    "        documents = loader.load()\n",
    "        print(f\"Loaded {len(documents)} pages from '{os.path.basename(book_path)}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading PDF: {e}\")\n",
    "        print(\"Please ensure the book path is correct and the PDF is not corrupted.\")\n",
    "        return []\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=2000,\n",
    "        chunk_overlap=300,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False,\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    print(f\"Split book into {len(chunks)} chunks.\")\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3588d3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 2. Create Embeddings and Vector Store ---\n",
    "def create_vector_store(chunks, embedding_model_name: str, faiss_path: str):\n",
    "    \"\"\"Creates or loads a FAISS vector store.\"\"\"\n",
    "    embeddings = SentenceTransformerEmbeddings(model_name=embedding_model_name)\n",
    "\n",
    "    if os.path.exists(faiss_path):\n",
    "        print(f\"Loading existing vector store from '{faiss_path}'...\")\n",
    "        vectorstore = FAISS.load_local(faiss_path, embeddings, allow_dangerous_deserialization=True)\n",
    "        print(\"Vector store loaded.\")\n",
    "    else:\n",
    "        print(\"Creating new vector store (this might take a moment)...\")\n",
    "        vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "        vectorstore.save_local(faiss_path)\n",
    "        print(\"Vector store created and saved.\")\n",
    "    return vectorstore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71b5fb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 3. Implement RAG ---\n",
    "def setup_rag_chain(llm_model_name: str, retriever):\n",
    "    \"\"\"Sets up the RAG chain with the specified LLM and retriever.\"\"\"\n",
    "    try:\n",
    "        llm = Ollama(model=llm_model_name)\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing Ollama with model '{llm_model_name}': {e}\")\n",
    "        print(\"Please ensure the Ollama server is running and the model is pulled.\")\n",
    "        return None\n",
    "\n",
    "    rag_prompt_template = \"\"\"Use the following context to answer the question at the end.\n",
    "If you don't know the answer, state that you don't know, and do not make up an answer.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    RAG_PROMPT = PromptTemplate(\n",
    "        template=rag_prompt_template,\n",
    "        input_variables=[\"context\", \"question\"]\n",
    "    )\n",
    "\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=retriever,\n",
    "        return_source_documents=True, # Set to True to get the retrieved chunks\n",
    "        chain_type_kwargs={\"prompt\": RAG_PROMPT}\n",
    "    )\n",
    "    return qa_chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d8b361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Chatbot...\n",
      "Loaded 1059 pages from 'Encyclopedia of Machine Learning.pdf'.\n",
      "Split book into 2912 chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amith\\AppData\\Local\\Temp\\ipykernel_14852\\3511803016.py:4: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = SentenceTransformerEmbeddings(model_name=embedding_model_name)\n",
      "c:\\Users\\amith\\anaconda3\\envs\\torch_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\amith\\anaconda3\\envs\\torch_env\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\amith\\.cache\\huggingface\\hub\\models--sentence-transformers--all-mpnet-base-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Main Chatbot Logic ---\n",
    "def main():\n",
    "    print(\"Initializing Chatbot...\")\n",
    "\n",
    "    # Load and chunk documents\n",
    "    chunks = load_and_chunk_documents(BOOK_PATH)\n",
    "    if not chunks:\n",
    "        print(\"Exiting due to document loading error.\")\n",
    "        return\n",
    "\n",
    "    # Create/Load vector store\n",
    "    vectorstore = create_vector_store(chunks, EMBEDDING_MODEL_NAME, FAISS_INDEX_PATH)\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4}) # Retrieve top 4 relevant chunks\n",
    "\n",
    "    # Set up RAG chain\n",
    "    qa_chain = setup_rag_chain(OLLAMA_MODEL_NAME, retriever)\n",
    "    if not qa_chain:\n",
    "        print(\"Exiting due to RAG chain setup error.\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nChatbot initialized with '{OLLAMA_MODEL_NAME}'! Ask questions about your technical book.\")\n",
    "    print(\"Type 'exit' to quit.\")\n",
    "\n",
    "    while True:\n",
    "        user_query = input(\"\\nYour Question: \")\n",
    "        if user_query.lower() == 'exit':\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "\n",
    "        print(\"Chatbot thinking...\")\n",
    "        try:\n",
    "            result = qa_chain.invoke({\"query\": user_query})\n",
    "            chatbot_response = result[\"result\"]\n",
    "            source_documents = result.get(\"source_documents\", [])\n",
    "\n",
    "            print(\"\\n--- Chatbot Response ---\")\n",
    "            print(chatbot_response)\n",
    "\n",
    "            if source_documents:\n",
    "                print(\"\\n--- Sources Used (Top Retrieved Chunks) ---\")\n",
    "                for i, doc in enumerate(source_documents):\n",
    "                    # Attempt to get page number if available in metadata\n",
    "                    page_info = f\"Page: {doc.metadata.get('page') + 1}, \" if 'page' in doc.metadata else \"\"\n",
    "                    source_filename = os.path.basename(doc.metadata.get('source', 'Unknown'))\n",
    "                    print(f\"Chunk {i+1} ({page_info}Source: {source_filename}):\")\n",
    "                    print(f\"  {doc.page_content[:200]}...\") # Display first 200 chars of the chunk\n",
    "            print(\"------------------------\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred during response generation: {e}\")\n",
    "            print(\"Please check your Ollama server and model, and ensure your prompt is well-formed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c478c93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cb2124",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c4e408",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
